{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.values\n",
    "features = df.loc[:,df.columns != 'label'].values/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prate\\AppData\\Local\\conda\\conda\\envs\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "features_train, features_val, labels_train, labels_val = train_test_split(features,labels,train_size=0.95,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.from_numpy(features_train), torch.from_numpy(labels_train))\n",
    "valset  = TensorDataset(torch.from_numpy(features_val),torch.from_numpy(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model - Multi layer NN \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.l1 = nn.Linear(in_features=in_features,out_features=150)\n",
    "        self.l2 = nn.Linear(in_features=150,out_features=150)\n",
    "        self.l2 = nn.Linear(in_features=150,out_features=150)\n",
    "        self.l3 = nn.Linear(in_features=150,out_features=out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        nn.init.kaiming_normal_(self.l1.weight)\n",
    "        nn.init.kaiming_normal_(self.l2.weight)\n",
    "        nn.init.kaiming_normal_(self.l3.weight)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(self.relu(x))\n",
    "        x = self.l3(self.relu(x))\n",
    "        return x\n",
    "\n",
    "net = MultiLayerPerceptron(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropy loss\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "#optimizer\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)\n",
    "epochs = 5\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 Valaccuracy 0.9714285714285714 loss 3.299712989246473e-05\n",
      "Epoch 0 iteration 1000 Valaccuracy 0.9709523809523809 loss 0.0005260467296466231\n",
      "Epoch 0 iteration 2000 Valaccuracy 0.97 loss 0.00780906667932868\n",
      "Epoch 0 iteration 3000 Valaccuracy 0.97 loss 0.00041790009709075093\n",
      "Epoch 0 iteration 4000 Valaccuracy 0.9719047619047619 loss 0.0005401611560955644\n",
      "Epoch 0 iteration 5000 Valaccuracy 0.9714285714285714 loss 0.0005321502685546875\n",
      "Epoch 0 iteration 6000 Valaccuracy 0.9714285714285714 loss 0.0001697540283203125\n",
      "Epoch 0 iteration 7000 Valaccuracy 0.9709523809523809 loss 0.00013427734666038305\n",
      "Epoch 1 iteration 0 Valaccuracy 0.9704761904761905 loss 0.00016345977201126516\n",
      "Epoch 1 iteration 1000 Valaccuracy 0.9709523809523809 loss 1.659393274167087e-05\n",
      "Epoch 1 iteration 2000 Valaccuracy 0.9704761904761905 loss 0.015848446637392044\n",
      "Epoch 1 iteration 3000 Valaccuracy 0.9704761904761905 loss 1.5258789289873675e-06\n",
      "Epoch 1 iteration 4000 Valaccuracy 0.9704761904761905 loss 0.0002056121884379536\n",
      "Epoch 1 iteration 5000 Valaccuracy 0.9704761904761905 loss 0.02776489220559597\n",
      "Epoch 1 iteration 6000 Valaccuracy 0.9704761904761905 loss 0.00052642822265625\n",
      "Epoch 1 iteration 7000 Valaccuracy 0.9704761904761905 loss 0.0001434326113667339\n",
      "Epoch 2 iteration 0 Valaccuracy 0.9704761904761905 loss 0.00342979421839118\n",
      "Epoch 2 iteration 1000 Valaccuracy 0.9704761904761905 loss 0.00539741525426507\n",
      "Epoch 2 iteration 2000 Valaccuracy 0.9704761904761905 loss 0.0002405166596872732\n",
      "Epoch 2 iteration 3000 Valaccuracy 0.9704761904761905 loss 0.00033779145451262593\n",
      "Epoch 2 iteration 4000 Valaccuracy 0.9704761904761905 loss 9.55581635935232e-05\n",
      "Epoch 2 iteration 5000 Valaccuracy 0.9704761904761905 loss 0.013652706518769264\n",
      "Epoch 2 iteration 6000 Valaccuracy 0.9704761904761905 loss 0.00021457672119140625\n",
      "Epoch 2 iteration 7000 Valaccuracy 0.9704761904761905 loss 0.04274597018957138\n",
      "Epoch 3 iteration 0 Valaccuracy 0.9704761904761905 loss 0.0002311706484761089\n",
      "Epoch 3 iteration 1000 Valaccuracy 0.9704761904761905 loss 0.00041522979154251516\n",
      "Epoch 3 iteration 2000 Valaccuracy 0.9704761904761905 loss 0.0025276183150708675\n",
      "Epoch 3 iteration 3000 Valaccuracy 0.9704761904761905 loss 0.003421211149543524\n",
      "Epoch 3 iteration 4000 Valaccuracy 0.9704761904761905 loss 0.0030338286887854338\n",
      "Epoch 3 iteration 5000 Valaccuracy 0.9704761904761905 loss 0.0009492874378338456\n",
      "Epoch 3 iteration 6000 Valaccuracy 0.9704761904761905 loss 0.0009366989252157509\n",
      "Epoch 3 iteration 7000 Valaccuracy 0.9704761904761905 loss 0.004216956906020641\n",
      "Epoch 4 iteration 0 Valaccuracy 0.9704761904761905 loss 0.0015529632801190019\n",
      "Epoch 4 iteration 1000 Valaccuracy 0.9704761904761905 loss 0.01133718527853489\n",
      "Epoch 4 iteration 2000 Valaccuracy 0.9704761904761905 loss 0.0007772445678710938\n",
      "Epoch 4 iteration 3000 Valaccuracy 0.9704761904761905 loss 0.017005348578095436\n",
      "Epoch 4 iteration 4000 Valaccuracy 0.9704761904761905 loss 0.0010423660278320312\n",
      "Epoch 4 iteration 5000 Valaccuracy 0.9704761904761905 loss 1.602172778802924e-05\n",
      "Epoch 4 iteration 6000 Valaccuracy 0.9704761904761905 loss 0.00023365020751953125\n",
      "Epoch 4 iteration 7000 Valaccuracy 0.9704761904761905 loss 0.00010070800635730848\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "train_loss = []\n",
    "for e in range(epochs):\n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        scheduler.step()\n",
    "\n",
    "        #wrapper\n",
    "        img = Variable(img.view(-1,28*28))\n",
    "        label = Variable(label).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = net(img)\n",
    "        loss = criteria(out, label)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %1000 ==0:\n",
    "            accuracy,correct = 0,0\n",
    "            net.eval()\n",
    "            for val_img, val_label in val_loader:\n",
    "                \n",
    "                #wrapper\n",
    "                val_img = Variable(val_img.view(-1,28*28))\n",
    "                \n",
    "                outs = net(val_img)\n",
    "                \n",
    "                predictions = torch.max(outs.data,1)[1]\n",
    "                correct += torch.sum(predictions==val_label.type(torch.LongTensor))\n",
    "\n",
    "            accuracy = 1.*correct.item()/float(len(valset))\n",
    "            print('Epoch {} iteration {} Valaccuracy {} loss {}'.format(e, i, accuracy, loss.item()))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('./test.csv', dtype=np.float32)\n",
    "test_features = torch.from_numpy(testdf.values)\n",
    "out = net(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = torch.max(out,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "testdf = pd.DataFrame({'ImageId': testdf.index.values+1,'Label':test_predictions.numpy()})\n",
    "testdf.to_csv('./pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
